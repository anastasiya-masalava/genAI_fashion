{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bbb9iU0DgQXT"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9ThH_CkhDDM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "BATCH_SIZE = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hj-n3IWhGqa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import clip\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz6mEcPbhIYV"
      },
      "outputs": [],
      "source": [
        "# Mount drive and set paths\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiPv89-jh0vc"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJxHg-tShzd1"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import h5py\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import clip\n",
        "from PIL import Image\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def load_mat_file(file_path):\n",
        "  try:\n",
        "    return scipy.io.loadmat(file_path)\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading {file_path}: {e}\")\n",
        "    return None\n",
        "\n",
        "class CLIPDataset(Dataset):\n",
        "  def __init__(self, anno_mat_path, image_h5_path, transform=None):\n",
        "    self.model, self.preprocess = clip.load(\"ViT-B/32\")\n",
        "    if transform is None:\n",
        "      self.transform = self.preprocess\n",
        "    else:\n",
        "      self.transform = transform\n",
        "    self.file_path = image_h5_path\n",
        "    self.anno_mat_path = anno_mat_path\n",
        "    anno_data = load_mat_file(self.anno_mat_path)\n",
        "    self.images = None\n",
        "    self.texts = anno_data.get('engJ', None)\n",
        "    if self.texts is None:\n",
        "      raise ValueError(\"No 'engJ' found in .mat file.\")\n",
        "    self.original_texts = [text[0] for text in self.texts]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = self.texts[idx][0][0]\n",
        "    if self.images is None:\n",
        "      self.images = h5py.File(self.file_path, 'r')[\"ih\"]\n",
        "    raw_image = self.images[idx]\n",
        "    if raw_image.shape[0] == 3:\n",
        "      raw_image = np.transpose(raw_image, (2, 1, 0))\n",
        "    if raw_image.dtype == np.float32 and raw_image.max() <= 1.0:\n",
        "      raw_image = (raw_image * 255).astype(np.uint8)\n",
        "    elif raw_image.dtype != np.uint8:\n",
        "      raw_image = raw_image.astype(np.uint8)\n",
        "    pil_image = Image.fromarray(raw_image)\n",
        "    transformed_image = self.transform(pil_image)\n",
        "    return transformed_image, text, idx\n",
        "\n",
        "def evaluate_clip_accuracy(model, dataset, dataloader, device=\"mps\", batch_size=16, k_values=[1, 5, 10]):\n",
        "  model.eval()\n",
        "  all_image_features = []\n",
        "  all_text_features = []\n",
        "  all_indices = []\n",
        "  all_texts = []\n",
        "  print(\"Extracting features\")\n",
        "  with torch.no_grad():\n",
        "    for images, texts, indices in tqdm(dataloader):\n",
        "      images = images.to(device)\n",
        "      indices = torch.tensor(indices)\n",
        "      image_features = model.encode_image(images)\n",
        "      text_tokens = clip.tokenize(texts).to(device)\n",
        "      text_features = model.encode_text(text_tokens)\n",
        "      image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "      text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "      all_image_features.append(image_features.cpu())\n",
        "      all_text_features.append(text_features.cpu())\n",
        "      all_indices.extend(indices.tolist())\n",
        "      all_texts.extend(texts)\n",
        "\n",
        "  all_image_features = torch.cat(all_image_features)\n",
        "  all_text_features = torch.cat(all_text_features)\n",
        "\n",
        "  print(\"Calculating similarity matrix\")\n",
        "  similarity_matrix = all_image_features @ all_text_features.T\n",
        "  print(\"Calculating accuracy metrics\")\n",
        "  results = {}\n",
        "\n",
        "  correct_at_k = {k: 0 for k in k_values}\n",
        "  ranks = []\n",
        "  for i, orig_idx in enumerate(all_indices):\n",
        "    similarities = similarity_matrix[i]\n",
        "    _, sorted_indices = similarities.sort(descending=True)\n",
        "    rank = (sorted_indices == i).nonzero().item() + 1\n",
        "    ranks.append(rank)\n",
        "    for k in k_values:\n",
        "      if rank <= k:\n",
        "        correct_at_k[k] += 1\n",
        "\n",
        "  for k in k_values:\n",
        "    results[f'top{k}_accuracy'] = correct_at_k[k] / len(all_indices) * 100\n",
        "    results[f'recall@{k}'] = correct_at_k[k] / len(all_indices) * 100\n",
        "\n",
        "  results['mean_rank'] = sum(ranks) / len(ranks)\n",
        "  results['median_rank'] = np.median(ranks)\n",
        "  best_matches = []\n",
        "  worst_matches = []\n",
        "\n",
        "  for i in range(min(5, len(all_indices))):\n",
        "    top_match = similarity_matrix[i].argmax().item()\n",
        "    score = similarity_matrix[i, top_match].item()\n",
        "    if top_match == i:\n",
        "      best_matches.append({\n",
        "        'image_idx': all_indices[i],\n",
        "        'text': all_texts[i],\n",
        "        'top_match_text': all_texts[top_match],\n",
        "        'score': score\n",
        "      })\n",
        "    else:\n",
        "      worst_matches.append({\n",
        "        'image_idx': all_indices[i],\n",
        "        'ground_truth_text': all_texts[i],\n",
        "        'predicted_text': all_texts[top_match],\n",
        "        'score': score,\n",
        "        'ground_truth_rank': ranks[i]\n",
        "      })\n",
        "  best_matches = sorted(best_matches, key=lambda x: x['score'], reverse=True)[:5]\n",
        "  worst_matches = sorted(worst_matches, key=lambda x: x['ground_truth_rank'], reverse=True)[:5]\n",
        "\n",
        "  results['best_matches'] = best_matches\n",
        "  results['worst_matches'] = worst_matches\n",
        "  return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  random.seed(42)\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "  print(f\"Using device: {device}\")\n",
        "\n",
        "  model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "  full_dataset = CLIPDataset(\"/content/drive/MyDrive/DL/Project/Fashion Synthesis Benchmark/Anno/language_original.mat\", \"/content/drive/MyDrive/DL/Project/G2.h5\")\n",
        "\n",
        "  total_len = len(full_dataset)\n",
        "  subset_size = int(total_len * 0.30)\n",
        "\n",
        "  print(f\"Full dataset size: {total_len}\")\n",
        "  print(f\"Evaluating on a 30% subset: {subset_size} samples\")\n",
        "\n",
        "  subset_indices = random.sample(range(total_len), subset_size)\n",
        "\n",
        "  subset_dataset = Subset(full_dataset, subset_indices)\n",
        "\n",
        "  batch_size = BATCH_SIZE\n",
        "  dataloader = DataLoader(\n",
        "      subset_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=2,\n",
        "      collate_fn=lambda batch: (\n",
        "      torch.stack([item[0] for item in batch]),\n",
        "      [item[1] for item in batch],\n",
        "      [item[2] for item in batch]\n",
        "    )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e7Tw6RLchzYM",
        "outputId": "f4239bc6-277e-453d-cc40-9f0ca9a8926a"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z_fyCKOh9l1"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import h5py\n",
        "import scipy.io\n",
        "\n",
        "def deepfashion_to_dataset(anno_path, image_h5_path, limit=None):\n",
        "    anno_data = scipy.io.loadmat(anno_path)\n",
        "    texts = [entry[0][0] for entry in anno_data['engJ']]\n",
        "\n",
        "    h5_file = h5py.File(image_h5_path, 'r')\n",
        "    images = h5_file['ih']\n",
        "\n",
        "    data = []\n",
        "    for i, text in enumerate(texts):\n",
        "        if limit and i >= limit:\n",
        "            break\n",
        "        raw_image = images[i]\n",
        "        if raw_image.shape[0] == 3:\n",
        "            raw_image = np.transpose(raw_image, (2, 1, 0))\n",
        "        image = Image.fromarray((raw_image * 255).astype(np.uint8))  # convert to proper image\n",
        "        data.append({\"image\": image, \"text\": text})\n",
        "\n",
        "    return Dataset.from_list(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYjyLQ0kh_6C"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples, vae, tokenizer, text_encoder):\n",
        "\n",
        "    transform = transforms.ToTensor()\n",
        "    images = [transform(image.convert(\"RGB\").resize((vae.config.sample_size, vae.config.sample_size))) for image in examples['image']]\n",
        "    images = torch.stack(images).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        latents = vae.encode(images).latent_dist.sample().detach()\n",
        "    latents = latents * 0.18215\n",
        "    latents = latents.half()\n",
        "\n",
        "    inputs = tokenizer(examples[\"text\"], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden_states = text_encoder(input_ids=inputs.input_ids.to(device), attention_mask=inputs.attention_mask.to(device)).last_hidden_state.half()\n",
        "\n",
        "    return {\n",
        "      \"pixel_values\": latents.cpu().numpy().tolist(),\n",
        "      \"input_ids\": inputs.input_ids.cpu().numpy().tolist(),\n",
        "      \"attention_mask\": inputs.attention_mask.cpu().numpy().tolist(),\n",
        "      \"encoder_hidden_states\": encoder_hidden_states.cpu().numpy().tolist()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8ghfXFyOiBYk"
      },
      "outputs": [],
      "source": [
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, StableDiffusionPipeline\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import torch\n",
        "\n",
        "def load_components():\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n",
        "  unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(device)\n",
        "  scheduler = PNDMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
        "  tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "  text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "\n",
        "  return vae, unet, scheduler, tokenizer, text_encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZFfr1d84O5l"
      },
      "outputs": [],
      "source": [
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inhmiiq4iC0v"
      },
      "outputs": [],
      "source": [
        "def get_data(DATASET_PATH, num_images, vae, tokenizer, text_encoder):\n",
        "  LOCAL_EXTRACT_DIR = '/content/extracted'\n",
        "  os.makedirs(LOCAL_EXTRACT_DIR, exist_ok=True)\n",
        "  anno_path = os.path.join(DATASET_PATH, 'Anno/language_original.mat')\n",
        "  eval_path = os.path.join(DATASET_PATH, 'Eval/ind.mat')\n",
        "  hf_dataset = deepfashion_to_dataset(anno_path, full_dataset.file_path, limit=num_images)  # for now just use 100 examples\n",
        "  preprocess_fn = partial(preprocess_function, vae=vae, tokenizer=tokenizer, text_encoder=text_encoder)\n",
        "  processed_dataset = hf_dataset.map(preprocess_fn, batched=True, batch_size=BATCH_SIZE)\n",
        "  processed_dataset = processed_dataset.remove_columns([\"image\", \"text\"])\n",
        "  processed_dataset.set_format(type=\"torch\")\n",
        "  sample = processed_dataset[0]\n",
        "  print({k: type(v) for k, v in sample.items()})\n",
        "\n",
        "  return processed_dataset, hf_dataset, anno_path, eval_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TR-ksaUy8dK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3dQ5LWi2x-h"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB-d3iEOxX1F"
      },
      "outputs": [],
      "source": [
        "def save_model_info(unet, tokenizer, text_encoder, scheduler):\n",
        "    custom_dir = \"/content/drive/MyDrive/DL/Project/project_models\"\n",
        "    os.makedirs(custom_dir, exist_ok=True)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "    save_dir = os.path.join(custom_dir, f\"finetuned_model_{timestamp}\")\n",
        "    zip_filename = os.path.join(custom_dir, f\"finetuned_model_{timestamp}.zip\")\n",
        "\n",
        "    unet.save_pretrained(os.path.join(save_dir, \"unet\"))\n",
        "    tokenizer.save_pretrained(os.path.join(save_dir, \"tokenizer\"))\n",
        "    text_encoder.save_pretrained(os.path.join(save_dir, \"text_encoder\"))\n",
        "    scheduler.save_config(os.path.join(save_dir, \"scheduler\"))\n",
        "\n",
        "    with ZipFile(zip_filename, 'w') as zipf:\n",
        "        for root, _, files in os.walk(save_dir):\n",
        "            for file in files:\n",
        "                filepath = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(filepath, save_dir)\n",
        "                zipf.write(filepath, arcname=os.path.join(f\"finetuned_model_{timestamp}\", arcname))\n",
        "\n",
        "    print(f\"Model saved and zipped to Google Drive: {zip_filename}\")\n",
        "    return zip_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVcJh8BYiPsq"
      },
      "outputs": [],
      "source": [
        "def train(DATASET_PATH, num_images, epochs, lr):\n",
        "  vae, unet, scheduler, tokenizer, text_encoder = load_components()\n",
        "  vae.to(device)\n",
        "  text_encoder.to(device)\n",
        "  unet.to(device)\n",
        "\n",
        "  vae.eval()\n",
        "  text_encoder.eval()\n",
        "\n",
        "  processed_dataset, hf_dataset, anno_path, eval_path = get_data(DATASET_PATH, num_images, vae, tokenizer, text_encoder)\n",
        "\n",
        "  train_dataloader = DataLoader(processed_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "  optimizer = torch.optim.AdamW(unet.parameters(), lr=lr)\n",
        "  unet.train()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      for batch in train_dataloader:\n",
        "          latents = batch[\"pixel_values\"].to(device)\n",
        "          encoder_hidden_states = batch[\"encoder_hidden_states\"].to(device)\n",
        "          noise = torch.randn_like(latents)\n",
        "          timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (latents.size(0),), device=device).long()\n",
        "          noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "          model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "          loss = torch.nn.functional.mse_loss(model_pred, noise)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "      print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "      del latents, noisy_latents, encoder_hidden_states, model_pred, loss\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  zip_filename = save_model_info(unet, tokenizer, text_encoder, scheduler)\n",
        "  return unet, tokenizer, text_encoder, scheduler, vae, hf_dataset, zip_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uQ8toFNd5FB"
      },
      "outputs": [],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_wx_Z5QWzylU",
        "outputId": "77984dac-540a-4f5c-b288-8f339c84344e"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import scipy.io\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1a3R0efNp1s"
      },
      "outputs": [],
      "source": [
        "def evaluate(eval_path, unet, tokenizer, text_encoder, scheduler, vae, hf_dataset, num_images, num_samples):\n",
        "  eval_data = scipy.io.loadmat(eval_path)\n",
        "  test_indices = []\n",
        "  for i in eval_data[\"test_ind\"].squeeze():\n",
        "    if i < num_images:\n",
        "      test_indices.append(int(i))\n",
        "\n",
        "\n",
        "  hf_test = hf_dataset.select(test_indices)\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(\n",
        "      \"CompVis/stable-diffusion-v1-4\",\n",
        "      vae=vae,\n",
        "      unet=unet,\n",
        "      text_encoder=text_encoder,\n",
        "      tokenizer=tokenizer,\n",
        "      scheduler=scheduler,\n",
        "      torch_dtype=torch.float16,\n",
        "\n",
        "  ).to(device)\n",
        "  pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
        "\n",
        "  scores = []\n",
        "\n",
        "  for i in tqdm(range(num_samples)):\n",
        "      text_prompt = hf_test[i][\"text\"]\n",
        "      generated_image = pipe(text_prompt).images[0]\n",
        "\n",
        "      image_input = preprocess_clip(generated_image).unsqueeze(0).to(device).to(torch.float32)\n",
        "      text_input = clip.tokenize([text_prompt]).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          image_features = clip_model.encode_image(image_input)\n",
        "          text_features = clip_model.encode_text(text_input)\n",
        "\n",
        "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "      similarity = (image_features @ text_features.T).item()\n",
        "\n",
        "      scores.append(similarity)\n",
        "\n",
        "  print(\"Average CLIP similarity score:\", round(sum(scores) / len(scores), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxAxrk4ioYxm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "from diffusers import UNet2DConditionModel, DDPMScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ITZkY-_oXj1"
      },
      "outputs": [],
      "source": [
        "def load_model_info(zip_path):\n",
        "    unzip_dir = os.path.splitext(zip_path)[0]\n",
        "    if not os.path.exists(unzip_dir):\n",
        "        from zipfile import ZipFile\n",
        "        with ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(os.path.dirname(zip_path))\n",
        "\n",
        "    unet = UNet2DConditionModel.from_pretrained(os.path.join(unzip_dir, \"unet\"))\n",
        "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(unzip_dir, \"tokenizer\"))\n",
        "    text_encoder = AutoModel.from_pretrained(os.path.join(unzip_dir, \"text_encoder\"))\n",
        "    scheduler = DDPMScheduler.from_pretrained(os.path.join(unzip_dir, \"scheduler\"))\n",
        "\n",
        "    print(f\"Model loaded from {zip_path}\")\n",
        "    return unet, tokenizer, text_encoder, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYqUZ27ZFlhl"
      },
      "outputs": [],
      "source": [
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, StableDiffusionPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkccQsdWFQvt"
      },
      "outputs": [],
      "source": [
        "def evaluate_from_saved(eval_path, zipfile_path, hf_dataset, num_images, num_samples):\n",
        "  eval_data = scipy.io.loadmat(eval_path)\n",
        "  test_indices = []\n",
        "  for i in eval_data[\"test_ind\"].squeeze():\n",
        "    if i < num_images:\n",
        "      test_indices.append(int(i))\n",
        "\n",
        "\n",
        "  hf_test = hf_dataset.select(test_indices)\n",
        "\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
        "  vae, unet, scheduler, tokenizer, text_encoder = load_components()\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    zipfile_path,\n",
        "    vae=vae,\n",
        "    unet=unet,\n",
        "    text_encoder=text_encoder,\n",
        "    tokenizer=tokenizer,\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float16,\n",
        "\n",
        "  )\n",
        "\n",
        "  pipe = pipe.to(device)\n",
        "  pipe.run_safety_checker = lambda images, device, dtype: (images, [False] * len(images))\n",
        "\n",
        "\n",
        "  scores = []\n",
        "\n",
        "  for i in tqdm(range(num_samples)):\n",
        "      text_prompt = hf_test[i][\"text\"]\n",
        "      generated_image = pipe(text_prompt).images[0]\n",
        "\n",
        "      image_input = preprocess_clip(generated_image).unsqueeze(0).to(device).to(torch.float32)\n",
        "      text_input = clip.tokenize([text_prompt]).to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          image_features = clip_model.encode_image(image_input)\n",
        "          text_features = clip_model.encode_text(text_input)\n",
        "\n",
        "      image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "      text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "      similarity = (image_features @ text_features.T).item()\n",
        "\n",
        "      scores.append(similarity)\n",
        "\n",
        "  print(\"Average CLIP similarity score:\", round(sum(scores) / len(scores), 4))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFtzifO372gI"
      },
      "source": [
        "# **Testing: num_images = 20**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6agQQguI9Rk"
      },
      "source": [
        "**training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgwKlIfH-MyA"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/content/drive/MyDrive/DL/Project/Fashion Synthesis Benchmark'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42wM0hFV9SIl"
      },
      "outputs": [],
      "source": [
        "unet, tokenizer, text_encoder, scheduler, vae, hf_dataset, zip_filename = train(DATASET_PATH, 20, 5, 5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0410tRBDIZyz"
      },
      "source": [
        "zipfile path: /content/drive/MyDrive/DL/Project/project_models/finetuned_model_20250425_2302.zip\n",
        "\n",
        "Loss Values:\n",
        "\n",
        "Epoch 1, Loss: 0.0646\n",
        "\n",
        "Epoch 2, Loss: 0.0465\n",
        "\n",
        "Epoch 3, Loss: 0.0438\n",
        "\n",
        "Epoch 4, Loss: 0.0634\n",
        "\n",
        "Epoch 5, Loss: 0.0197"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDZaNGKXNg9u"
      },
      "outputs": [],
      "source": [
        "eval_path = \"/content/drive/MyDrive/DL/Project/Fashion Synthesis Benchmark/Eval/ind.mat\"\n",
        "evaluate(eval_path, unet, tokenizer, text_encoder, scheduler, vae, hf_dataset, 20, 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFQhfwQ_OTV1"
      },
      "source": [
        "Average CLIP similarity score: 0.2829"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyJje8-2ObK5"
      },
      "source": [
        "# **5 epochs batch size 8 num_images 100**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIJ8wnIvOjXR"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/content/drive/MyDrive/DL/Project/Fashion Synthesis Benchmark'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MI91svgOqSL"
      },
      "outputs": [],
      "source": [
        "unet, tokenizer, text_encoder, scheduler, vae, hf_dataset, zip_filename = train(DATASET_PATH, 100, 5, 5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_jJsokSPZkR"
      },
      "source": [
        "zipfile path: /content/drive/MyDrive/DL/Project/project_models/finetuned_model_20250425_2316.zip\n",
        "\n",
        "Losses:\n",
        "\n",
        "Epoch 1, Loss: 0.0389\n",
        "\n",
        "Epoch 2, Loss: 0.0374\n",
        "\n",
        "Epoch 3, Loss: 0.1419\n",
        "\n",
        "Epoch 4, Loss: 0.0394\n",
        "\n",
        "Epoch 5, Loss: 0.0118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0bQ1XhEOy5h"
      },
      "outputs": [],
      "source": [
        "eval_path = \"/content/drive/MyDrive/DL/Project/Fashion Synthesis Benchmark/Eval/ind.mat\"\n",
        "evaluate(eval_path, unet, tokenizer, text_encoder, scheduler, vae, hf_dataset, 100, 90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m1wHs-xOkAL"
      },
      "source": [
        "Average CLIP similarity score: 0.2564"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-0uiRgZQlzj"
      },
      "source": [
        "# **5 epochs, batch size 8, 1000 images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Co5blaQQxZ6"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/content/drive/MyDrive/DL/Project/Fashion Synthesis Benchmark'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPIIt6OeQzFD"
      },
      "outputs": [],
      "source": [
        "unet, tokenizer, text_encoder, scheduler, vae, hf_dataset, zip_filename = train(DATASET_PATH, 1000, 5, 5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDmVl4ShTNFC"
      },
      "source": [
        "zipfile path:/content/drive/MyDrive/DL/Project/project_models/finetuned_model_20250425_2333.zip\n",
        "\n",
        "Losses:\n",
        "\n",
        "Epoch 1, Loss: 0.0748\n",
        "\n",
        "Epoch 2, Loss: 0.0763\n",
        "\n",
        "Epoch 3, Loss: 0.0283\n",
        "\n",
        "Epoch 4, Loss: 0.0639\n",
        "\n",
        "Epoch 5, Loss: 0.0262"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oi8cFqQNGAPh"
      },
      "outputs": [],
      "source": [
        "zipfolder_path = '/content/drive/MyDrive/DL/Project/project_models/finetuned_model_20250425_2302'\n",
        "eval_path = \"/content/drive/MyDrive/DL/Project/Fashion Synthesis Benchmark/Eval/ind.mat\"\n",
        "hf_dataset = deepfashion_to_dataset(anno_path, full_dataset.file_path, limit=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9frFgmxF-rl"
      },
      "outputs": [],
      "source": [
        "evaluate_from_saved(eval_path, zipfolder_path, hf_dataset, 1000, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TftLMdeZQ6DW"
      },
      "source": [
        "# **5 epochs, batch size 8, 10000 images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvshxRwGRA0R"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/content/drive/MyDrive/DL/Project/Fashion Synthesis Benchmark'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h87DbwzdRBWX"
      },
      "outputs": [],
      "source": [
        "unet, tokenizer, text_encoder, scheduler, vae, hf_dataset, zip_filename = train(DATASET_PATH, 10000, 5, 5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO_Wk-dKRH3Z"
      },
      "source": [
        "# **5 epochs, batch size 8, 20000 images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6hjKduqROzL"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = '/content/drive/MyDrive/DL/Project/Fashion Synthesis Benchmark'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncAeWBIQRPU_"
      },
      "outputs": [],
      "source": [
        "unet, tokenizer, text_encoder, scheduler, vae, hf_dataset, zip_filename = train(DATASET_PATH, 20000, 5, 5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PpML8aHD_C1"
      },
      "source": [
        "# Loss Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9D2OSqfEBhr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCibzyrbEE0p"
      },
      "outputs": [],
      "source": [
        "def plot_loss_table(loss_lists, num_images_list, batch_size):\n",
        "  epochs = len(loss_lists[0])\n",
        "  columns = [\"Number of Images\"] + [f\"Epoch {i+1} loss\" for i in range(epochs)]\n",
        "  rows = [str(images) for images in num_images_list]\n",
        "\n",
        "  cell = []\n",
        "  for img, losses in zip(rows, loss_lists):\n",
        "      formatted_losses = [f\"{loss:.4f}\" for loss in losses]\n",
        "      cell.append([img] + formatted_losses)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(1+epochs, len(rows)))\n",
        "\n",
        "  #actual table\n",
        "  table = ax.table(\n",
        "      cellText=cell,\n",
        "      colLabels=columns,\n",
        "      loc='center',\n",
        "      cellLoc='center'\n",
        "  )\n",
        "\n",
        "  table.scale(2, 2)\n",
        "\n",
        "  ax.axis(\"off\")\n",
        "\n",
        "  plt.title(f\"Loss per Epoch at Batch Size {batch_size} for Different Number of Images\", fontsize=14)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjLeOywSJriD"
      },
      "outputs": [],
      "source": [
        "def plot_loss_line_graph(loss_lists, num_images_list, batch_size):\n",
        "    epochs = len(loss_lists[0])\n",
        "    epochs_list = list(range(1, epochs + 1))\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    for losses, num_images in zip(loss_lists, num_images_list):\n",
        "        plt.plot(epochs_list, losses, marker='o', label=f\"{num_images} images\")\n",
        "    plt.xlabel(\"Epoch\", fontsize=12)\n",
        "    plt.ylabel(\"Loss\", fontsize=12)\n",
        "    plt.title(f\"Loss vs Epochs for Different Number of Images at Batch Size {batch_size}\", fontsize=14)\n",
        "    plt.legend(title=\"Dataset Size\")\n",
        "    plt.grid(True)\n",
        "    plt.xticks(epochs_list)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Titpd5mE_Pw"
      },
      "outputs": [],
      "source": [
        "loss_lists_batch_8 = [[0.0646, 0.0465, 0.0438, 0.0634, 0.0197],\n",
        "                      [0.0389, 0.0374, 0.1419, 0.0394, 0.0118],\n",
        "                      [0.0748, 0.0763, 0.0283, 0.0639, 0.0262]]\n",
        "\n",
        "num_images_batch_8 = [20, 100, 1000]\n",
        "plot_loss_table(loss_lists_batch_8, num_images_batch_8, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TsE6lbSJ_Zg"
      },
      "outputs": [],
      "source": [
        "plot_loss_line_graph(loss_lists_batch_8, num_images_batch_8, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be5O1uDKw6-d"
      },
      "source": [
        "### Experiements\n",
        "\n",
        "### Failure on CUDA\n",
        "Batch size = 16, Epoch = 3\n",
        "Batch size = 32, Epoch = 3\n",
        "\n",
        "#### 3 epochs & batch size = 8 ->\n",
        "Epoch 1, Loss: 0.0597\n",
        "\n",
        "Epoch 2, Loss: 0.0617\n",
        "\n",
        "Epoch 3, Loss: 0.1440\n",
        "\n",
        "Average CLIP similarity score: 0.2369\n",
        "\n",
        "\n",
        "#### 5 epochs & batch size = 8 ->\n",
        "Epoch 1, 0.0398\n",
        "\n",
        "Epoch 2, 0.0768\n",
        "\n",
        "Epoch 3, 0.0316\n",
        "\n",
        "Epoch 4, 0.0576\n",
        "\n",
        "Epoch 5, 0.0356\n",
        "\n",
        "Average CLIP Similarity score: 0.2296"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3v4VRGEepr"
      },
      "source": [
        "# Generating Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk1ZUTX7Ed5f"
      },
      "outputs": [],
      "source": [
        "vae, unet, scheduler, tokenizer, text_encoder = load_components()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCBW6DCDEj-c"
      },
      "outputs": [],
      "source": [
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, StableDiffusionPipeline\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"/content/drive/MyDrive/GT/Deep Learning/Project/project_models/finetuned_model_20250425_2302\",\n",
        "    vae=vae,\n",
        "    unet=unet,\n",
        "    text_encoder=text_encoder,\n",
        "    tokenizer=tokenizer,\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float16,\n",
        "\n",
        ")\n",
        "\n",
        "pipe = pipe.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9yjCmG3El0s"
      },
      "outputs": [],
      "source": [
        "prompt_list = [\n",
        "    \"navy basketball shorts and shoes\", \"red dress and black heels\", \"denim jacket\", \"yellow summer dress\", \"white graduation dress\",\n",
        "    \"black sweatsuit\", \"navy suit\", \"leather pants and jacket\", \"cocktail dress\", \"short sleeve hawaiian shirt\",\n",
        "    \"girls pajamas\", \"boys pajamas\", \"pink soccer uniform\", \"irish costume\", \"college party dress\", \"cowboy boots\", \"rain boots\",\n",
        "    \"women's corporate blouse\", \"swimming trunks for men\", \"gray hoodie and joggers\", \"black leather skirt\", \"white blouse\",\n",
        "    \"green cargo pants\", \"blue denim jeans\", \"formal black dress\", \"plaid button-up shirt\", \"pink ballet dress\", \"long-sleeve sweater\",\n",
        "    \"fuzzy winter coat\", \"cheetah print leggings\", \"blue tank top\", \"faux fur vest\", \"red plaid skirt\", \"tropical floral dress\",\n",
        "    \"yellow raincoat\", \"striped sweater\", \"beige trench coat\", \"pink hoodie\", \"sporty windbreaker\", \"high-waisted jeans\",\n",
        "    \"v-neck t-shirt\", \"knitted scarf\", \"short overalls\", \"black leather boots\", \"slim fit suit\", \"sundress\", \"black mini skirt\",\n",
        "    \"sweater dress\", \"white tennis shoes\", \"men's athletic shorts\", \"purple velvet pants\", \"checkered flannel shirt\", \"brown leather belt\",\n",
        "    \"fleece jacket\", \"graphic t-shirt\", \"long-sleeve turtleneck\", \"blue denim shorts\", \"high-neck sweater\", \"white tank top\",\n",
        "    \"pleated skirt\", \"denim overalls\", \"red leather jacket\", \"chinos\", \"beanie hat\", \"vintage denim jacket\", \"red satin dress\",\n",
        "    \"white button-down shirt\", \"knee-high boots\", \"cargo shorts\", \"green parka\", \"chambray shirt\", \"tie-dye hoodie\", \"black cargo pants\",\n",
        "    \"pleated trousers\", \"floral romper\", \"denim skirt\", \"leopard print blouse\", \"yellow blouse\", \"thermal leggings\",\n",
        "    \"gray plaid pants\", \"sporty sneakers\", \"off-the-shoulder blouse\", \"red crop top\", \"white tennis skirt\", \"gray cargo shorts\",\n",
        "    \"cozy fleece sweater\", \"black leggings\", \"tailored blazer\", \"high-top sneakers\", \"leather ankle boots\", \"light denim jacket\",\n",
        "    \"red bomber jacket\", \"striped leggings\", \"denim vest\", \"black cargo shorts\", \"leopard print jacket\", \"open-back dress\",\n",
        "    \"yellow floral blouse\", \"denim mini skirt\", \"puffer jacket\", \"wrap dress\", \"black knee-high socks\", \"brown boots\",\n",
        "    \"white sports bra\", \"floral blouse\", \"blue cardigan\", \"black dress pants\", \"sweatshirt and leggings\", \"gray hoodie and jeans\",\n",
        "    \"fitted t-shirt\", \"yellow skirt\", \"blue zip-up hoodie\", \"orange beanie\", \"beige shorts\", \"leather biker jacket\",\n",
        "    \"blush pink dress\", \"beige boots\", \"black high-waisted skirt\", \"lumberjack shirt\", \"fuzzy slippers\", \"polo shirt\",\n",
        "    \"warm winter sweater\", \"light pink scarf\", \"beige cardigan\", \"long skirt\", \"cashmere sweater\", \"black slip dress\",\n",
        "    \"tartan skirt\", \"brown cargo pants\", \"camel coat\", \"striped t-shirt\", \"white sneakers\", \"blue overalls\",\n",
        "    \"gray wool coat\", \"checked pants\", \"flannel pajama set\", \"turtleneck sweater\", \"green bomber jacket\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksDAY82xEnvL"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "images = []\n",
        "\n",
        "num_inference_steps = 50\n",
        "guidance_scale = 7.5\n",
        "\n",
        "for prompt in prompt_list:\n",
        "  with torch.autocast(\"cuda\"):\n",
        "      output = pipe(\n",
        "          prompt,\n",
        "          num_inference_steps=num_inference_steps,\n",
        "          guidance_scale=guidance_scale,\n",
        "      )\n",
        "\n",
        "  image = output.images[0]\n",
        "  images.append(image)\n",
        "  display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6FNc3_HEqJ8"
      },
      "outputs": [],
      "source": [
        "ind = 117\n",
        "print(prompt_list[ind])\n",
        "display(images[ind])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuNqQ-ISEre5"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import inception_v3\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def calculate_inception_score(images, batch_size=32, splits=10, device=None):\n",
        "    \"\"\"\n",
        "    Calculate the Inception Score for a list of images\n",
        "\n",
        "    Args:\n",
        "        images: List of PIL images or tensor of shape [N, 3, H, W] in range [0, 1]\n",
        "        batch_size: Batch size for inference\n",
        "        splits: Number of splits to compute the score\n",
        "        device: Device to run the model on\n",
        "\n",
        "    Returns:\n",
        "        mean and standard deviation of the Inception Score\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    inception_model = inception_v3(pretrained=True, transform_input=False)\n",
        "    inception_model.eval()\n",
        "    inception_model.to(device)\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    if isinstance(images[0], Image.Image):\n",
        "        processed_images = []\n",
        "        for img in images:\n",
        "            processed_images.append(preprocess(img))\n",
        "        images = torch.stack(processed_images)\n",
        "\n",
        "    class ImageDataset(Dataset):\n",
        "        def __init__(self, images):\n",
        "            self.images = images\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.images)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.images[idx]\n",
        "\n",
        "    dataloader = DataLoader(ImageDataset(images), batch_size=batch_size)\n",
        "\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = batch.to(device)\n",
        "            pred = inception_model(batch)\n",
        "            pred = F.softmax(pred, dim=1)\n",
        "            preds.append(pred.cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "\n",
        "    mean_preds = np.mean(preds, axis=0)\n",
        "\n",
        "    scores = []\n",
        "    for i in range(splits):\n",
        "        part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n",
        "        kl_div = part * (np.log(part) - np.log(mean_preds[None, :]))\n",
        "        kl_div = np.sum(kl_div, axis=1)\n",
        "        scores.append(np.exp(np.mean(kl_div)))\n",
        "\n",
        "    return np.mean(scores), np.std(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkzENc1GEtxQ"
      },
      "outputs": [],
      "source": [
        "score_mean, score_std = calculate_inception_score(images)\n",
        "print(f\"Inception Score: {score_mean} ± {score_std}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKrvp0lbEuTu"
      },
      "source": [
        "Inception Score: 8.114767074584961 ± 1.0872924327850342"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
