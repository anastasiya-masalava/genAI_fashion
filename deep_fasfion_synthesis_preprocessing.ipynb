{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "AAgQ7qrgbZhf"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trEzMJBWeXsF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from google.colab import drive\n",
    "import zipfile\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "import clip\n",
    "import scipy.io\n",
    "import h5py\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5tq9UeKPOv1W"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "DATASET_PATH = '/content/drive/MyDrive/Fashion Synthesis Benchmark' #update to your directory***\n",
    "\n",
    "LOCAL_EXTRACT_DIR = '/content/extracted'\n",
    "os.makedirs(LOCAL_EXTRACT_DIR, exist_ok=True)\n",
    "\n",
    "anno_path = os.path.join(DATASET_PATH, 'Anno/language_original.mat')\n",
    "eval_path = os.path.join(DATASET_PATH, 'Eval/ind.mat')\n",
    "\n",
    "def load_mat_file(file_path):\n",
    "    \"\"\"Load a MAT file and return its contents\"\"\"\n",
    "    try:\n",
    "        return scipy.io.loadmat(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_fashion_attributes(anno_data):\n",
    "    \"\"\"Analyze fashion dataset attributes and create a structured DataFrame\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    attributes = ['cate_new', 'color_', 'gender_', 'sleeve_']\n",
    "\n",
    "    for attr in attributes:\n",
    "        if attr in anno_data:\n",
    "            df[attr.rstrip('_').replace('cate_new', 'category')] = [\n",
    "                int(val[0]) for val in anno_data[attr]\n",
    "            ]\n",
    "\n",
    "    if 'nameList' in anno_data:\n",
    "\n",
    "      df['filename'] = [\n",
    "          name[0][0].decode('utf-8') if hasattr(name[0][0], 'decode') else str(name[0][0])\n",
    "          for name in anno_data['nameList']\n",
    "      ]\n",
    "\n",
    "    df = add_text_labels(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_text_labels(df):\n",
    "    \"\"\"Add text labels for numerical category, color, gender, and sleeve codes\"\"\"\n",
    "    gender_mapping = {0: 'Female', 1: 'Male'}\n",
    "    sleeve_mapping = {1: 'Short', 2: 'Medium', 3: 'Long', 4: 'Other'}\n",
    "\n",
    "    # note -- these are just educated guess on based on dataset description \n",
    "    category_mapping = {\n",
    "        1: 'T-Shirt', 2: 'Blouse', 3: 'Tank Top', 4: 'Sweater',\n",
    "        5: 'Suit/Blazer', 6: 'Jacket', 7: 'Vest', 8: 'Coat',\n",
    "        9: 'Dress Shirt', 10: 'Polo Shirt', 11: 'Cardigan', 12: 'Dress',\n",
    "        13: 'Skirt', 14: 'Shorts', 15: 'Sweatshirt', 16: 'Pants/Trousers',\n",
    "        17: 'Jeans', 18: 'Hoodie', 19: 'Leggings'\n",
    "    }\n",
    "\n",
    "    color_mapping = {\n",
    "        1: 'Black', 2: 'White', 3: 'Blue', 4: 'Red', 6: 'Grey',\n",
    "        8: 'Green', 9: 'Yellow', 12: 'Brown', 13: 'Pink', 17: 'Multicolor/Pattern'\n",
    "    }\n",
    "\n",
    "    df['gender_text'] = df['gender'].map(gender_mapping).fillna('Unknown')\n",
    "    df['sleeve_text'] = df['sleeve'].map(sleeve_mapping).fillna('Unknown')\n",
    "    df['category_text'] = df['category'].map(category_mapping).fillna('Category-Unknown')\n",
    "    df['color_text'] = df['color'].map(color_mapping).fillna('Color-Unknown')\n",
    "\n",
    "    return df\n",
    "\n",
    "def visualize_dataset(df):\n",
    "    \"\"\"Create visualizations of the dataset\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    category_counts = df['category_text'].value_counts().head(10)\n",
    "    sns.barplot(x=category_counts.index, y=category_counts.values)\n",
    "    plt.title('Top 10 Clothing Categories')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    gender_counts = df['gender_text'].value_counts()\n",
    "    sns.barplot(x=gender_counts.index, y=gender_counts.values)\n",
    "    plt.title('Gender Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sleeve_counts = df['sleeve_text'].value_counts()\n",
    "    sns.barplot(x=sleeve_counts.index, y=sleeve_counts.values)\n",
    "    plt.title('Sleeve Type Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    color_counts = df['color_text'].value_counts()\n",
    "\n",
    "    color_map = {\n",
    "        'Black': '#000000',\n",
    "        'White': '#FFFFFF',\n",
    "        'Blue': '#0000FF',\n",
    "        'Red': '#FF0000',\n",
    "        'Grey': '#808080',\n",
    "        'Green': '#008000',\n",
    "        'Yellow': '#FFFF00',\n",
    "        'Brown': '#A52A2A',\n",
    "        'Pink': '#FFC0CB',\n",
    "        'Multicolor/Pattern': '#7F00FF',\n",
    "        'Color-Unknown': '#CCCCCC'\n",
    "    }\n",
    "\n",
    "    bar_colors = [color_map.get(color, '#CCCCCC') for color in color_counts.index]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(x=color_counts.index, y=color_counts.values, palette=bar_colors)\n",
    "    plt.title('Color Distribution in Fashion Dataset')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    for i, color in enumerate(color_counts.index):\n",
    "        if color == 'White':\n",
    "            bar = ax.patches[i]\n",
    "            bar.set_edgecolor('black')\n",
    "            bar.set_linewidth(1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    cat_gender_cross = pd.crosstab(df['category_text'], df['gender_text'], normalize='index')\n",
    "    sns.heatmap(cat_gender_cross, annot=True, cmap=\"YlGnBu\", fmt='.2f')\n",
    "    plt.title('Gender Distribution Across Categories')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    color_cat_cross = pd.crosstab(df['color_text'], df['category_text'], normalize='index')\n",
    "\n",
    "    top_categories = df['category_text'].value_counts().head(10).index\n",
    "    color_cat_cross = color_cat_cross[top_categories]\n",
    "    sns.heatmap(color_cat_cross, annot=True, cmap=\"YlOrRd\", fmt='.2f')\n",
    "    plt.title('Color Distribution Across Top 10 Categories')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the analysis pipeline\"\"\"\n",
    "    anno_data = load_mat_file(anno_path)\n",
    "    eval_data = load_mat_file(eval_path)\n",
    "    df = analyze_fashion_attributes(anno_data)\n",
    "    visualize_dataset(df)\n",
    "    df.to_csv('deepfashion_processed.csv', index=False)\n",
    "    return df\n",
    "\n",
    "df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "GLGGNU9PPmw8"
   },
   "outputs": [],
   "source": [
    "def analyze_descriptions_with_nlp(anno_data):\n",
    "    \"\"\"Use NLP techniques to extract style and attribute information from descriptions\"\"\"\n",
    "\n",
    "    if 'engJ' not in anno_data:\n",
    "        print(\"engJ field not found in annotation data\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        import nltk\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        from nltk.corpus import stopwords\n",
    "        from collections import Counter\n",
    "\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "\n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords')\n",
    "    except ImportError:\n",
    "        print(\"NLTK not available. Install with: !pip install nltk\")\n",
    "        return\n",
    "\n",
    "    descriptions = []\n",
    "    for item in anno_data['engJ'].flatten():\n",
    "        if isinstance(item, np.ndarray) and item.size > 0:\n",
    "            text = item.flatten()[0]\n",
    "            if hasattr(text, 'decode'):\n",
    "                try:\n",
    "                    text = text.decode('utf-8')\n",
    "                except:\n",
    "                    continue\n",
    "            descriptions.append(text)\n",
    "        elif isinstance(item, str):\n",
    "            descriptions.append(item)\n",
    "\n",
    "    if not descriptions:\n",
    "        print(\"No valid descriptions found\")\n",
    "        return\n",
    "\n",
    "    print(f\"Analyzing {len(descriptions)} text descriptions\")\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    all_tokens = []\n",
    "    for desc in descriptions:\n",
    "        tokens = word_tokenize(desc.lower())\n",
    "        filtered_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "        all_tokens.extend(filtered_tokens)\n",
    "\n",
    "    word_counts = Counter(all_tokens)\n",
    "\n",
    "    print(\"\\nMost common words in descriptions:\")\n",
    "    for word, count in word_counts.most_common(20):\n",
    "        print(f\"  {word}: {count}\")\n",
    "\n",
    "    try:\n",
    "        from wordcloud import WordCloud\n",
    "\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of Fashion Descriptions')\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"WordCloud not available. Install with: !pip install wordcloud\")\n",
    "\n",
    "    bigrams = []\n",
    "    for desc in descriptions:\n",
    "        tokens = word_tokenize(desc.lower())\n",
    "        filtered_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "        for i in range(len(filtered_tokens) - 1):\n",
    "            bigrams.append((filtered_tokens[i], filtered_tokens[i+1]))\n",
    "\n",
    "    bigram_counts = Counter(bigrams)\n",
    "\n",
    "    print(\"\\nMost common two-word phrases:\")\n",
    "    for bigram, count in bigram_counts.most_common(15):\n",
    "        print(f\"  {bigram[0]} {bigram[1]}: {count}\")\n",
    "\n",
    "    return word_counts, bigram_counts\n",
    "\n",
    "anno_data = load_mat_file(anno_path)\n",
    "eval_data = load_mat_file(eval_path)\n",
    "word_counts, bigram_counts = analyze_descriptions_with_nlp(anno_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIQy2dRQdRPk"
   },
   "source": [
    "**Clip Dataset** for loading and preprocessing images and their text descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxmPR9PHJT9b"
   },
   "outputs": [],
   "source": [
    "def load_mat_file(file_path):\n",
    "  \"\"\"Load a MAT file and return its contents\"\"\"\n",
    "  try:\n",
    "    return scipy.io.loadmat(file_path)\n",
    "  except Exception as e:\n",
    "    print(f\"Error loading {file_path}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmY9qk8IK0AS"
   },
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "  def __init__(self, anno_mat_path, image_h5_path, transform=None):\n",
    "    self.model, self.preprocess = clip.load(\"ViT-B/32\")\n",
    "    if transform is None:\n",
    "      self.transform = self.preprocess\n",
    "    else:\n",
    "      self.transform = transform\n",
    "    self.file_path = image_h5_path\n",
    "    self.anno_mat_path = anno_mat_path\n",
    "    anno_data = load_mat_file(self.anno_mat_path)\n",
    "    self.images = None\n",
    "    self.texts = anno_data.get('engJ', None)\n",
    "    if self.texts is None:\n",
    "      raise ValueError(\"No 'engJ' found in .mat file.\")\n",
    "    self.original_texts = [text[0] for text in self.texts]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = self.texts[idx][0][0]\n",
    "    if self.images is None:\n",
    "      self.images = h5py.File(self.file_path, 'r')[\"ih\"]\n",
    "    raw_image = self.images[idx]\n",
    "    if raw_image.shape[0] == 3:\n",
    "      raw_image = np.transpose(raw_image, (2, 1, 0))\n",
    "    if raw_image.dtype == np.float32 and raw_image.max() <= 1.0:\n",
    "      raw_image = (raw_image * 255).astype(np.uint8)\n",
    "    elif raw_image.dtype != np.uint8:\n",
    "      raw_image = raw_image.astype(np.uint8)\n",
    "    pil_image = Image.fromarray(raw_image)\n",
    "    transformed_image = self.transform(pil_image)\n",
    "    return transformed_image, text, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wS4sYKB7Mzmg"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  random.seed(42)\n",
    "  torch.manual_seed(42)\n",
    "  device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "  print(f\"Using device: {device}\")\n",
    "\n",
    "  model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "  full_dataset = CLIPDataset(\"/content/drive/MyDrive/DeepLearning/TeamProject/Fashion Synthesis Benchmark/Anno/language_original.mat\", \"/content/drive/MyDrive/DeepLearning/TeamProject/G2.h5\")\n",
    "  total_len = len(full_dataset)\n",
    "  subset_size = int(total_len * 0.30)\n",
    "\n",
    "  print(f\"Full dataset size: {total_len}\")\n",
    "  print(f\"Evaluating on a 30% subset: {subset_size} samples\")\n",
    "\n",
    "  subset_indices = random.sample(range(total_len), subset_size)\n",
    "  subset_dataset = Subset(full_dataset, subset_indices)\n",
    "\n",
    "  batch_size = 16\n",
    "  dataloader = DataLoader(\n",
    "      subset_dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=2,\n",
    "      collate_fn=lambda batch: (\n",
    "      torch.stack([item[0] for item in batch]),\n",
    "      [item[1] for item in batch],\n",
    "      [item[2] for item in batch]\n",
    "    )\n",
    "  )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
